#!/usr/bin/env python3
'''Training and test set generation code.

Introduction
------------

The default evaluation method used by Nostril is based on precomputing the
frequencies of n-grams in real words and random strings.  This requires a
corpus of strings to build the training sets.  This module (`training.py`)
contains the code to create the training set and generate sets of random
strings with similar lengths as those in the training set.

Generating the training set
---------------------------

The training set consists of a mix of three kinds of strings:

  1) Real words and character strings appearing in selected text corpora.
  2) Strings generated by concatenating randomly-selected strings from #1.
  3) Program identifiers obtained from randomly selected GitHub repos.

About (1)

The words in part (1) constitute both a set of real string to train on, and
a starting set for creating the strings in part (2).  Some explanations are
warranted about how (1) is created.  Initially I used /usr/share/dict/web2
as a list of words (and also tried NLTK's words.words(), but it's basically
the same).  Unfortunately, this led to surprisingly poor classification
performance.  Examining NLTK's word.words() and the dictionary revealed
they have a lot of extremely unusual and uncommon words, and some of them
*look* like random character strings (to me).  I surmised that this
produced a noisy training set that confused the classifier.  As a result, I
proceeded to create a new list of words more systematically by using
hand-picked corpora.  The current word selection approach is this:

  1) Select words longer than 4 characters from the following corpora:

    - Words from a sample of NTLK corpora that have more or less
      contemporary English text.

    - Words from two of the "problem report" files in NTLK's set of
      corpora.  These add some more technical terms and include some
      program code text for added realism.

    - The free 5000 word list from the Corpus of Contemporary American
      English, available from http://www.wordfrequency.info (as it existed
      on 2016-11-22).

  2) Add a selection of common English stop words (and, not, in, few, etc.).
     This is done in a separate step because the stop words are shorter than
     the 4-character minimum limit.

It is possible that performance could be improved further by adding more
corpuses.  I tried a bunch more from NLTK, but many of them either use
older English, or have business or other orientations that do not (to me)
seem as relevant to our projects aim of analyzing software repositories,
or added very few new words on top of what is already being used.

The point of limiting the length of text selections to 4 characters is to
try to limit the appearance of short names and acronyms, based on the
(admittedly untested) expectation that this would likely increase noise in
the training corpus.  The addition of some common stop words is needed
because software developers often use them in identifier names
("notEnabled", "setAndReturn", etc.).  We add them separately because they
are shorter than the 4-character limit and so would be filtered out
otherwise.

About (2)

To this list of words (and some program identifiers) obtained from the
selected NLTK corpora and other sources, we add a large set of strings
created by concatenating words selected at random from the starting set.
This has the effect of creating many more strings that look like program
identifiers constructed out of regular words.  They are admittedly more
random than real identifiers would be (e.g., you end up with things like
"nighlysailing", "nervouslyexpandproperties", etc.); on the other hand,
software can be about anything, and programmers are often creative in their
choice of words, so it does not seem *too* unrealistic to take this
approach.  In any case, the notion of identifiers being composed of
multiple words is normal, and this has been the only way I could think of
to produce a large corpus of them.

About (3)

Finally, to the list of strings constructed above, we add a selection of
real program identifiers obtained from a process of extracting text from
randomly selected GitHub repositories.  They number far fewer than the
strings constructed by the procedure above (3500 at the time of this
writing), but are real examples of what this system is intended to
distinguish.

Authors
-------

Michael Hucka <mhucka@caltech.edu>

Copyright
---------

Copyright (c) 2017-2019 by the California Institute of Technology.  This
software was developed as part of the CASICS project, the Comprehensive and
Automated Software Inventory Creation System. For more, visit http://casics.org.
'''

import os
import random
import re
import string
import sys

if '__file__' in globals():
    thisdir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(os.path.join(thisdir, '../common'))
    sys.path.append(os.path.join(thisdir, '../splitters'))
    sys.path.append(os.path.join(thisdir, '../extractor'))
else:
    sys.path.append('../common')
    sys.path.append('../splitters')
    sys.path.append('../extractor')

from utils import *
from simple_splitters import simple_split, delimiter_split
from text_extractor import is_word
from nonsense_detector import dataset_from_pickle


# Word corpus generators.
# .............................................................................

def english_word_list(min_length=4):
    '''Returns a list of words to use as a corpus of real words.'''
    raw = set()
    raw.update(_words_from_nltk())
    raw.update(_words_from_file('words-byu-free-corpus.txt'))
    raw.update(_words_from_file('nltk-linux-problem-reports.txt'))
    raw.update(_words_from_file('nltk-apache-problem-reports.txt'))
    raw.update(_words_from_file('other-words.txt'))
    # Remove words shorter than the threshold.
    semi_final = [w for w in raw if len(w) >= min_length]
    # Add back common stopwords that are often used in identifier strings.
    # This is done after removing short words because these stop words are
    # all very short.  (This relies on knowing that the way the output from
    # this function will be used is in a step that will concatenate multiple
    # words from the list.  Thus, the short length of stop words is okay in
    # this situation.)
    semi_final += _words_from_file('common-stop-words.txt')
    return semi_final


_delims = str.maketrans('-/._', '    ')

def _words_from_nltk():
    import nltk
    words = set()
    words.update(nltk.corpus.words.words('en-basic'))
    words.update(nltk.corpus.brown.words(categories='science_fiction'))
    words.update(nltk.corpus.brown.words(categories='reviews'))
    words.update(nltk.corpus.reuters.words(categories=['cpu', 'jobs', 'retail']))
    words.update(nltk.corpus.webtext.words())
    masc_corpora = ['written/1471-230X-2-21.txt', 'written/wsj_2465.txt',
                    'written/ch5.txt', 'written/wsj_0173.txt',
                    'written/wsj_0173.txt',
                    'written/blog-varsity-athletics.txt',
                    'written/lessig_blog-carbon.txt',
                    'written/JurassicParkIV-INT.txt',
                    'written/JurassicParkIV-Scene_1.txt',
                    'written/JurassicParkIV-Scene_3.txt',
                    'written/JurassicParkIV-Scene_4.txt',
                    'written/JurassicParkIV-Scene_5.txt',
                    'written/blog-jet-lag.txt', 'written/Ant_Robot.txt',
                    'written/Earnings_Season_Underway.txt',
                    'written/110CYL067.txt', 'written/111344.txt']
    for corpus in masc_corpora:
        words.update(nltk.corpus.masc_tagged.words(corpus))
    words = flatten(x.translate(_delims).split(' ') for x in words)
    words = [x for x in words if x.isalpha()]
    # Some of the tokens in the webtext corpus (and maybe others) include
    # program identifiers.  Here we split them using a reasonably
    # conservative splitting algorithm and put the individual words in the
    # list.  This does mean we may get word fragments, but then again, that
    # arguably results in a more realistic training set.
    words = flatten(simple_split(w) for w in words)
    words = [x for x in words if is_word(x)]
    return words


def _words_from_file(filename, subdir='training/word-corpora'):
    words = set()
    file = full_path(filename, subdir)
    with open(file, 'r') as f:
        for word in f.read().strip().splitlines():
            word = word.strip()
            if not word or word.startswith('#'):
                continue
            if word.isalpha():
                words.add(word)
    # Split hyphenated words into separate words.
    words = list(flatten(x.translate(_delims).split(' ') for x in words))
    return words


# Identifier data sets
# .............................................................................

def identifier_list(min_length=4):
    '''Returns a list of program identifiers from real sources.'''
    identifiers = set()
    identifiers.update(_identifiers_from_file('random-identifiers-from-github.txt'))
    identifiers.update(_identifiers_from_file('selected-linux-symbols.txt'))
    # Remove bad chars & identifiers shorter than the threshold.
    identifiers = flatten(id.translate(_delims).split(' ') for id in identifiers)
    return [id for id in identifiers if len(id) >= min_length]


def _identifiers_from_file(filename, subdir='training/identifier-corpora'):
    words = set()
    file = full_path(filename, subdir)
    count = 0
    with open(file, 'r') as f:
        for word in f.read().strip().splitlines():
            word = word.strip()
            if not word or word.startswith('#'):
                continue
            words.add(word)
            count += 1
    return list(words)


# Training set generator
# .............................................................................
#
# Usage:
#
# I experimented with a lot of values.  The best performance I was able to
# find came with using a training set of 3,000,000 strings with
# max_concat_words = 2, and 4-grams (in nonsense_detector.py).  Here is the
# sequence of how I used this and the n-gram values computation:
#
#    words = english_word_list()
#    ids   = identifier_list()
#    ts    = training_set(words, ids, 3000000, 2)
#    freq  = ngram_values(ts, 4)
#
# Note: it's useful to save the training set in a pickle file, so that it
# does not have to be regenerated every time you need it.
#
#    dataset_to_pickle('training/training_set.pklz', ts)
#
# For testing, you will also want to generate a set of random strings, and
# save that to a file too.  It takes the list of training_set words so that
# the random strings it generates will have the same length distribution and
# the total set will have the same size as the training set.
#
#    rs = random_set(ts)
#    dataset_to_pickle('tests/random_set.pklz', rs)

def training_set(word_list=None, identifier_list=None,
                 total_cases=3000000, max_concat_words=2, min_length=4,
                 pickle_file='training/training_set.pklz'):
    '''Generate a training set for the nonsense detector.
    Words are drawn from 'word_list' at random.  Words shorter than
    'min_length' are skipped.  The value of 'max_concat_words' indicates the
    max number of real words to concatenate together.  The algorithm randomly
    picks a number between 2 and max_concat_words each time it creates a new
    word.

    If 'word_list' is not given, then this function instead looks for the
    file named by 'pickle_file', reads it, and returns the contents.

    Note that it is possible that the number of cases returned may be less
    than 'total_cases' if the starting list of words doesn't have enough to
    create 'total_cases' total strings of at most 'max_words'.
    '''
    import pickle
    if not word_list:
        file = full_path(pickle_file)
        if not os.path.exists(file):
            raise ValueError('Cannot find pickle file {}'.format(file))
        return dataset_from_pickle(file)
    # Sanity check our arguments and try to do something reasonable.
    if len(identifier_list) > total_cases:
        return identifiers[:total_cases]
    if len(identifier_list) + len(word_list) > total_cases:
        how_many_more = total_cases - len(identifier_list)
        return identifier_list + word_list[:how_many_more]
    # Create a set, because we want to avoid duplicates.
    strings = set()
    strings.update(identifier_list)
    # Add all the individual words.
    for word in word_list:
        # We don't leave words shorter than min_length in the set.
        if len(word) < min_length:
            continue
        strings.add(word)
    # Generate strings consisting of randomly concatenated strings.
    while len(strings) < total_cases:
        # Note we don't check for minimum length at this point.  We want
        # shorter words like stopwords to be included in combinations of words,
        # even though we don't want them by themselves (and hence, the test
        # earlier for min_length).
        num_words = random.randrange(2, max_concat_words + 1)
        candidate = _word_string(word_list, num_words)
        if candidate in strings:
            # Don't repeat things.
            continue
        strings.add(candidate)
    return list(strings)


def random_set(training_set=None,
               pickle_file='tests/unlabeled-cases/random_set.pklz'):
    '''Generate a set of random text strings.  The argument 'ts' should be a
    training set; the random strings will have the same lengths as the
    strings in the training set, and there will be approximately the same
    number of strings as the size of the training set.
    '''
    import pickle
    if not training_set:
        file = full_path(pickle_file)
        if not os.path.exists(file):
            raise ValueError('Cannot find pickle file {}'.format(file))
        return dataset_from_pickle(file)
    strings = set()
    for s in training_set:
        strings.add(_random_string(len(s)))
    return list(strings)


def _word_string(word_list, num_words=1):
    total_words = len(word_list)
    return ''.join(word_list[random.randrange(0, total_words)].lower()
                   for i in range(num_words))


def _random_string(length):
    # letters = string.ascii_lowercase + string.ascii_uppercase
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(0, length))
